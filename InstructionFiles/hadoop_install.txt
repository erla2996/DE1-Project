Installation intructions for Hadoop, on Ubuntu 20 LTS.

Based (mostly) on:
	https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html
	https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/ClusterSetup.html
	https://www.linode.com/docs/guides/how-to-install-and-set-up-hadoop-cluster/

# Prerequisites

### Update Apt
	sudo apt-get update

### Install java 8
The java version was decided by looking at https://cwiki.apache.org/confluence/display/HADOOP/Hadoop+Java+Versions
	sudo apt-get install openjdk-8-jdk

### SSH
Install the following:
	sudo apt-get install ssh
	sudo apt-get install pdsh

### Download Hadoop

If we already have the .tar.gz archive containing Hadoop on one of the VMs, you can use scp to move the .tar.gz to the new machine, and unzip it with
	tar -xvzf hadoop-3.2.2.tar.gz

Hadoop can be downloaded from https://hadoop.apache.org/releases.html. Choose the 3.2.2 version. Once downloaded, verify the file using the instructions at the site. 
Or directly download by mirror from https://mirror.jframeworks.com/apache/hadoop/common/hadoop-3.2.2/ .
Transfer the verified .tar.gz archive to the VM, and unzip.


### Modify env variables
Navigate to the unzipped folder hadoop-3.2.2/
	cd hadoop-3.2.2
Use vim to modify etc/hadoop/hadoop-env.sh. Change line 54 to
	export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64/jre
Remember to clean up the # sign
Test this by running
	bin/hadoop
If it works, you should see usage documentation for the hadoop script.
If you get an output saying that "ERROR: JAVA_HOME ... does not exist.", try "readlink -f $(which java)" to find the java location on your system and substitute the output. Hadoop requires that "JAVA_HOME/bin/java" exists.

# Local (Standalone) Mode
The system is now configured to run in a non-distributed mode.

# Setup pseudo-distributed operation
You need to do this before setting up fully distributed mode.

### Change Hadoop configs
Change the contents of etc/hadoop/core-site.xml to
	<configuration>
	    <property>
		<name>fs.defaultFS</name>
		<value>hdfs://localhost:9000</value>
	    </property>
	</configuration>
and the contents of etc/hadoop/hdfs-site.xml to
	<configuration>
	    <property>
		<name>dfs.replication</name>
		<value>1</value>
	    </property>
	</configuration>

### Setup passphraseless ssh
Try
	ssh localhost
If you cannot connect, run the following commands:
	ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa
	cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
	chmod 0600 ~/.ssh/authorized_keys
You should now be able to connect to localhost with "ssh localhost"

Check the pdsh default rcmd:
	pdsh -q -w localhost | grep "Rcmd type"
If it is anything other than "ssh", append the following line to "~/.bashrc", using e.g. vim:
	export PDSH_RCMD_TYPE=ssh
And run the following:
	source ~/.bashrc

You should now be able to start the hdfs in local (standalone) mode using
	sbin/start-dfs.sh
and stop it using
	sbin/stop-dfs.sh

# Fully-Distributed Mode

Install Hadoop on each machine, by following the above instructions.

### Setup passphraseless ssh
Transfer the public key you created earlier to the other machines using scp. Place the .pub in ~/.ssh/
	scp -i DE1-Team1.pem .ssh/id_rsa.pub ubuntu@XXX.XXX.X.XX:~/.ssh/t.pub
Add the key to ~/.ssh/authorized_keys on the other machines:
	cat ~/.ssh/t.pub >> ~/.ssh/authorized_keys
	chmod 0600 ~/.ssh/authorized_keys
	rm t.pub
Test that this works by using ssh to enter the machine (without providing DE1-Team1.pem).


### Edit /etc/hosts
Edit the file /etc/hosts on each VM, by adding lines for each node in the cluster.
	192.168.2.98 node-master
	192.168.2.78 node-worker-1
	and so on...


## Configure the master node / namenode

### Set environment variables
In ~/.bashrc, append the following:
	export HADOOP_HOME=/home/ubuntu/hadoop-3.2.2
	export PATH=${PATH}:${HADOOP_HOME}/bin:${HADOOP_HOME}/sbin
And run the following:
	source ~/.bashrc

### Config files
On the node-master machine, do the following:
Change the contents of etc/hadoop/core-site.xml to:
	<configuration>
	        <property>
	                <name>fs.default.name</name>
	                <value>hdfs://node-master:9000</value>
	        </property>
	</configuration>
Edit the contents of etc/hadoop/hdfs-site.xml to:
	<configuration>
	    <property>
	            <name>dfs.namenode.name.dir</name>
	            <value>/home/ubuntu/data/nameNode</value>
	    </property>

	    <property>
	            <name>dfs.datanode.data.dir</name>
	            <value>/home/ubuntu/data/dataNode</value>
	    </property>

	    <property>
	            <name>dfs.replication</name>
	            <value>1</value>
	    </property>
	</configuration>
Edit the etc/hadoop/mapred-site.xml:
	<configuration>
	    <property>
	            <name>mapreduce.framework.name</name>
	            <value>yarn</value>
	    </property>
	    <property>
	            <name>yarn.app.mapreduce.am.env</name>
	            <value>HADOOP_MAPRED_HOME=$HADOOP_HOME</value>
	    </property>
	    <property>
	            <name>mapreduce.map.env</name>
	            <value>HADOOP_MAPRED_HOME=$HADOOP_HOME</value>
	    </property>
	    <property>
	            <name>mapreduce.reduce.env</name>
	            <value>HADOOP_MAPRED_HOME=$HADOOP_HOME</value>
	    </property>
		 <property>
		        <name>yarn.app.mapreduce.am.resource.mb</name>
		        <value>512</value>
		</property>

		<property>
		        <name>mapreduce.map.memory.mb</name>
		        <value>256</value>
		</property>

		<property>
		        <name>mapreduce.reduce.memory.mb</name>
		        <value>256</value>
		</property>
	</configuration>
Edit etc/hadoop/yarn-site.xml:
	<configuration>
	    <property>
	            <name>yarn.acl.enable</name>
	            <value>0</value>
	    </property>
	    <property>
	            <name>yarn.resourcemanager.hostname</name>
	            <value>192.168.2.98</value>
	    </property>
	    <property>
	            <name>yarn.nodemanager.aux-services</name>
	            <value>mapreduce_shuffle</value>
	    </property>
		 <property>
		        <name>yarn.nodemanager.resource.memory-mb</name>
		        <value>1536</value>
		</property>

		<property>
		        <name>yarn.scheduler.maximum-allocation-mb</name>
		        <value>1536</value>
		</property>

		<property>
		        <name>yarn.scheduler.minimum-allocation-mb</name>
		        <value>128</value>
		</property>

		<property>
		        <name>yarn.nodemanager.vmem-check-enabled</name>
		        <value>false</value>
		</property>
	</configuration>
Edit etc/hadoop/workers:
	node-worker-1

# Configure the worker nodes

### Copy the config files from the master node to the worker nodes
SSH into to the master node.
Run
	scp ~/hadoop-3.2.2/etc/hadoop/* ubuntu@node-worker-1:~/hadoop-3.2.2/etc/hadoop/

# Format HDFS
On the master node, run
	~/hadoop-3.2.2/bin/hdfs namenode -format

You can now start the HDFS by running
	start-dfs.sh
and stop it by running
	stop-dfs.sh
